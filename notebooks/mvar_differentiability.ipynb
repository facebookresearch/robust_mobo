{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Revisiting the idea of obtaining MVaR derivatives via component-wise mapping.\n",
    "\n",
    "First, we need to re-implement the approximate derivatives. Then, we can compare\n",
    "it with the finite-difference estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[2.1189, 2.6644],\n",
      "          [1.7094, 2.6702]]]], grad_fn=<ViewBackward>)\n",
      "(tensor([[[5.5261, 6.5328]]]),)\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "from botorch.acquisition.multi_objective.multi_output_risk_measures import MVaR\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class DiffMVaR(MVaR):\n",
    "\n",
    "    def make_diffable(self, prepared_samples: Tensor, mvar_set: List[Tensor]) -> List[Tensor]:\n",
    "        r\"\"\"An experimental approach for obtaining the gradient of the MVaR via\n",
    "        component-wise mapping to original samples.\n",
    "\n",
    "        Args:\n",
    "            prepared_samples: A `(sample_shape * batch_shape * q) x n_w x m`-dim tensor of\n",
    "                posterior samples. The q-batches should be ordered so that each\n",
    "                `n_w` block of samples correspond to the same input.\n",
    "            mvar_set: A `(sample_shape * batch_shape * q)` list of `k x m`-dim tensor\n",
    "                of MVaR values, where `k` is varies depending on the particular batch.\n",
    "\n",
    "        Returns:\n",
    "            The same `mvar_set` with entries mapped to inputs to produce gradients.\n",
    "        \"\"\"\n",
    "        for batch_idx in range(prepared_samples.shape[0]):\n",
    "            base_samples = prepared_samples[batch_idx]\n",
    "            mvars = mvar_set[batch_idx]\n",
    "            equal_check = mvars.unsqueeze(-2) == base_samples\n",
    "            new_mvars_list = []\n",
    "            for check in equal_check:\n",
    "                p1 = base_samples[check[:, 0], 0].mean()\n",
    "                p2 = base_samples[check[:, 1], 1].mean()\n",
    "                new_mvars_list.append(torch.stack([p1, p2]))\n",
    "            mvar_set[batch_idx] = torch.stack(new_mvars_list)\n",
    "        return mvar_set\n",
    "\n",
    "\n",
    "    def forward(self, samples: Tensor, X: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Calculate the MVaR corresponding to the given samples.\n",
    "\n",
    "        Args:\n",
    "            samples: A `sample_shape x batch_shape x (q * n_w) x m`-dim tensor of\n",
    "                posterior samples. The q-batches should be ordered so that each\n",
    "                `n_w` block of samples correspond to the same input.\n",
    "            X: A `batch_shape x q x d`-dim tensor of inputs. Ignored.\n",
    "\n",
    "        Returns:\n",
    "            A `sample_shape x batch_shape x q x m`-dim tensor of MVaR values,\n",
    "            if `self.expectation=True`.\n",
    "            Otherwise, this returns a `sample_shape x batch_shape x (q * k') x m`-dim\n",
    "            tensor, where `k'` is the maximum `k` across all batches that is returned\n",
    "            by `get_mvar_set_...`. Each `(q * k') x m` corresponds to the `k` MVaR\n",
    "            values for each `q` batch of `n_w` inputs, padded up to `k'` by repeating\n",
    "            the last element. If `self.pad_to_n_w`, we set `k' = self.n_w`, producing\n",
    "            a deterministic return shape.\n",
    "        \"\"\"\n",
    "        batch_shape, m = samples.shape[:-2], samples.shape[-1]\n",
    "        prepared_samples = self._prepare_samples(samples)\n",
    "        # This is -1 x n_w x m.\n",
    "        prepared_samples = prepared_samples.reshape(-1, *prepared_samples.shape[-2:])\n",
    "        # Get the mvar set using the appropriate method based on device, m & n_w.\n",
    "        # NOTE: The `n_w <= 64` part is based on testing on a 24 core CPU.\n",
    "        # `get_mvar_set_gpu` heavily relies on parallelized batch computations and\n",
    "        # may scale worse on CPUs with fewer cores.\n",
    "        # Using `no_grad` here since `MVaR` is not differentiable.\n",
    "        with torch.no_grad():\n",
    "            if (\n",
    "                samples.device == torch.device(\"cpu\")\n",
    "                and m == 2\n",
    "                and prepared_samples.shape[-2] <= 64\n",
    "            ):\n",
    "                mvar_set = self.get_mvar_set_cpu(prepared_samples)\n",
    "            else:\n",
    "                mvar_set = self.get_mvar_set_gpu(prepared_samples)\n",
    "        if samples.requires_grad:\n",
    "            mvar_set = self.make_diffable(prepared_samples, mvar_set)\n",
    "        # Set the `pad_size` to either `self.n_w` or the size of the largest MVaR set.\n",
    "        pad_size = self.n_w if self.pad_to_n_w else max([_.shape[0] for _ in mvar_set])\n",
    "        padded_mvar_list = []\n",
    "        for mvar_ in mvar_set:\n",
    "            if self.expectation:\n",
    "                padded_mvar_list.append(mvar_.mean(dim=0))\n",
    "            else:\n",
    "                # Repeat the last entry to make `mvar_set` `n_w x m`.\n",
    "                repeats_needed = pad_size - mvar_.shape[0]\n",
    "                padded_mvar_list.append(\n",
    "                    torch.cat([mvar_, mvar_[-1].expand(repeats_needed, m)], dim=0)\n",
    "                )\n",
    "        mvars = torch.stack(padded_mvar_list, dim=0)\n",
    "        return mvars.view(*batch_shape, -1, m)\n",
    "\n",
    "\n",
    "def func(X: Tensor, n_w: int = 5, seed: int = 0) -> Tensor:\n",
    "    torch.manual_seed(seed)\n",
    "    perturbed_X = X.unsqueeze(-2) + torch.rand(n_w, X.shape[-1])\n",
    "    return perturbed_X.pow(2)\n",
    "\n",
    "X = torch.ones(1, 1, 2, requires_grad=True)\n",
    "Y = func(X)\n",
    "mvar = DiffMVaR(n_w=5, alpha=0.6)\n",
    "mvar_vals = mvar(Y)\n",
    "grad = torch.autograd.grad(mvar_vals.sum(), X)\n",
    "print(mvar_vals)\n",
    "print(grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(50.7030) tensor(50.6973, grad_fn=<DivBackward0>)\n",
      "tensor(-0.0056, grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.seed()\n",
    "X = torch.rand(3, 2, 2, requires_grad=True)\n",
    "eps = 1e-4\n",
    "X_eps = X + eps\n",
    "Y = func(X)\n",
    "mvar_Y = mvar(Y)\n",
    "Y_eps = func(X_eps)\n",
    "mvar_Y_eps = mvar(Y_eps)\n",
    "\n",
    "grad = torch.autograd.grad(mvar_Y.sum(), X)\n",
    "grad_fd = (mvar_Y_eps.sum() - mvar_Y.sum()) / eps\n",
    "# print(mvar_Y)\n",
    "print(grad[0].sum(), grad_fd)\n",
    "print(grad_fd - grad[0].sum())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1483.1537) tensor(1485.5957, grad_fn=<DivBackward0>)\n",
      "tensor(2.4420, grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from botorch.test_functions.multi_objective import DH3\n",
    "\n",
    "X = torch.rand(3, 2, 5, 3, requires_grad=True)\n",
    "func = DH3(dim=3)\n",
    "eps = 1e-4\n",
    "X_eps = X + eps\n",
    "Y = func(X)\n",
    "mvar_Y = mvar(Y)\n",
    "Y_eps = func(X_eps)\n",
    "mvar_Y_eps = mvar(Y_eps)\n",
    "\n",
    "grad = torch.autograd.grad(mvar_Y.sum(), X)\n",
    "grad_fd = (mvar_Y_eps.sum() - mvar_Y.sum()) / eps\n",
    "# print(mvar_Y)\n",
    "print(grad[0].sum(), grad_fd)\n",
    "print(grad_fd - grad[0].sum())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}